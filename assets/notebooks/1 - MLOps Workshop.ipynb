{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps Workshop - Git & Deployment\n",
    "\n",
    "In questo notebook esploreremo le best practices del deployment di modelli di Machine Learning su Cloud Pak for Data. \n",
    "\n",
    "Il workshop è suddiviso in due parti: la prima parte simulerà lo sviluppo di un modello di ML seguendo le guidelines Git Flow, mentre la seconda eseguirà il deployment del modello in un deployment space. \n",
    "\n",
    "Svilupperemo un semplice modello Tensorflow per la classificazione di cifre scritte a mano prese dal dataset MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General + Git setup\n",
    "\n",
    "Andiamo ad installare ed importare le librerie necessarie e ad impostare le variabili di ambiente per gestire il git flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54a9de420efe49079259a8e13769f0db",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Installazione wget e tensorflow_datasets \n",
    "\n",
    "!pip install -q wget\n",
    "!pip install -q tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a67f925c-4e09-4158-9133-21d150271bd8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import librerie\n",
    "\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import subprocess\n",
    "import wget\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impostiamo come variabili globali il nome del branch (CURRENT_BRANCH) preso direttamente dalle variabili di ambiente e il nome dell'environment (CURRENT_ENV), il quale può essere soltanto prd o dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90c5abf2-5ade-4de5-acf5-f2fbe7abf7ef",
    "tags": []
   },
   "outputs": [],
   "source": [
    "CURRENT_BRANCH = subprocess.run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'], stdout=subprocess.PIPE)\\\n",
    "    .stdout.strip().decode(\"utf-8\")\n",
    "\n",
    "if CURRENT_BRANCH in ['prd']:\n",
    "    CURRENT_ENV=CURRENT_BRANCH\n",
    "else:\n",
    "    CURRENT_ENV='dev'\n",
    "    \n",
    "print('Current branch     : {}'.format(CURRENT_BRANCH))\n",
    "print('Current environment: {}'.format(CURRENT_ENV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data setup\n",
    "\n",
    "Scarichiamo un'immagine di esempio per iniziare ad esplorare i dati "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c70470d01d224b568730406a9dad6039",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download del file di esempio e caricamento in una variabile\n",
    "\n",
    "sample_canvas_data_filename = 'mnist-html-canvas-image-data.json'\n",
    "url = 'https://github.com/IBM/watson-machine-learning-samples/raw/master/cloud/data/mnist/' + sample_canvas_data_filename\n",
    "if not os.path.isfile(sample_canvas_data_filename): wget.download(url)\n",
    "\n",
    "with open(sample_canvas_data_filename) as data_file:\n",
    "    sample_cavas_data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd2b136198314d1e8c8e9de29298244f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot dell'immagine con matplotlib\n",
    "\n",
    "print(\"Height (n): \" + str(sample_cavas_data[\"height\"]) + \" pixels\\n\")\n",
    "print(\"Num image data entries: \" + str(len( sample_cavas_data[\"data\"])) + \" - (n * n * 4) elements - RGBA values\\n\")\n",
    "\n",
    "rgba_arr = np.asarray(sample_cavas_data[\"data\"]).astype('uint8')\n",
    "n = sample_cavas_data[\"height\"]\n",
    "plt.figure()\n",
    "plt.imshow( rgba_arr.reshape(n, n, 4))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download del dataset completo da Tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02247a2d57a74a14877898ea71c0a840",
    "tags": []
   },
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    "    data_dir=\"../data_asset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizzazione e divisione del dataset in training e test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61a50fb413b44396bf2b828d3a594424",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25c6da45ee234c678175892ff368cd7a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definizione e training del modello\n",
    "\n",
    "Definiamo una rete composta da 2 layer Dense (fully connected) alla quale aggiungeremo un layer Lambda che faccia restituire la classificazione effettiva dell'immagine (e non le probabilità)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd10e95736784e978ce3756dc72dfe31",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)), # <- NON MODIFICARE\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(1, (3, 3), activation='relu'), # <- HINT\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(1, (3, 3), activation='relu'), # <- HINT\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10) # <- NON MODIFICARE\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=2,\n",
    "    validation_data=ds_test,\n",
    ")\n",
    "\n",
    "model.add(tf.keras.layers.Lambda(lambda x: tf.keras.backend.cast(tf.keras.backend.argmax(x), dtype='int32'), name='classes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testiamo il modello sul dataset di test (usato anche come validation) per vedere il formato dell'output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33de0bfe310446878fb2550da8a98b18",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salviamo il modello sul filesystem locale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac50e9508aa84ede82b994beaf2eb408",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path='../data_asset/mnist-dl-model'\n",
    "tf.saved_model.save(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "In questa parte andremo a fare il deploy del modello sviluppato, andandolo prima a salvare tra gli asset di un deployment space e poi facendo il deploy dello stesso.\n",
    "\n",
    "![DIAGRAM](img/diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials \n",
    "\n",
    "Andiamo a salvare in una variabile le credenziali per accedere al modello, tra cui l'access token. Queste variabili sono salvate dal CP4D direttamente nelle variabili di ambiente, per cui possiamo recuperarle in maniera rapida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61c2793abf6b4235affa4c1514d1184e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cpdtoken=os.environ['USER_ACCESS_TOKEN']\n",
    "\n",
    "wml_credentials = {\n",
    "    \"token\": cpdtoken,\n",
    "    \"instance_id\" : \"openshift\",\n",
    "    \"url\": os.environ['RUNTIME_ENV_APSX_URL'],\n",
    "    \"version\": \"4.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impostiamo il client di Watson Machine Learning con le credenziali rintracciate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "666cdcab44e44e268523d7a30a6de4ab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning import APIClient\n",
    "client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Space\n",
    "\n",
    "Troviamo i deployment space e impostiamo l'id nella variabile `space_id`. Se il deployment space non esiste ne viene creato uno in automatico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f31622c42e574053b0361be54d003880",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List deployment spaces\n",
    "\n",
    "client.spaces.list(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "256aa2da73e146838efc0abdd5d7f316",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO Change ID\n",
    "space_id = ''\n",
    "\n",
    "use_existing_space=True\n",
    "space_name = f'mlops-{CURRENT_ENV}'\n",
    "\n",
    "# Codice per creare il deployment space da codice\n",
    "\n",
    "for space in client.spaces.get_details()['resources']:\n",
    "\n",
    "    if space['entity']['name'] == space_name:\n",
    "        print(\"Deployment space with name\",space_name,\"already exists . .\")\n",
    "        space_id=space['metadata']['id']\n",
    "        client.set.default_space(space_id)\n",
    "        if use_existing_space==False:\n",
    "\n",
    "            for deployment in client.deployments.get_details()['resources']:\n",
    "                print(\"Deleting deployment\",deployment['entity']['name'], \"in the space\",)\n",
    "                deployment_id=deployment['metadata']['id']\n",
    "                client.deployments.delete(deployment_id)\n",
    "            print(\"Deleting Space \",space_name,)\n",
    "            client.spaces.delete(space_id)\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            print(\"Using the existing space\")\n",
    "\n",
    "if (space_id == \"\" or use_existing_space == False):\n",
    "    print(\"\\nCreating a new deployment space - \",space_name)\n",
    "    space_meta_data = {\n",
    "        client.spaces.ConfigurationMetaNames.NAME : space_name\n",
    "    }\n",
    "\n",
    "    stored_space_details = client.spaces.store(space_meta_data)\n",
    "\n",
    "    space_id = stored_space_details['metadata']['id']\n",
    "\n",
    "client.set.default_space(space_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Model\n",
    "\n",
    "Salviamo il modello nel repository, ovvero lo spazio 'Assets' del Deployment Space. Creiamo anche una revision per poter versionare il modello.\n",
    "\n",
    "Se il model ID non viene passato o non viene trovato all'interno del deployment space, andiamo a crearne uno nuovo e ad assegnare una nuova revision all'asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Change ID\n",
    "\n",
    "model_id = \"\"\n",
    "model_exist = False\n",
    "\n",
    "if model_id != \"\":\n",
    "    for entity in client.repository.get_details()['models']['resources']:\n",
    "        model_exist = entity['metadata']['id'] == model_id or model_exist\n",
    "\n",
    "    assert model_exist, \"ID modello non esiste\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exist:\n",
    "    sofware_spec_uid = client.software_specifications.get_id_by_name(\"tensorflow_rt22.1-py3.9\")\n",
    "    metadata = {\n",
    "                client.repository.ModelMetaNames.NAME: \"MNIST Tensorflow Model\",\n",
    "                client.repository.ModelMetaNames.TYPE: 'tensorflow_2.7',\n",
    "                client.repository.ModelMetaNames.TAGS: ['OCR'],\n",
    "                client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sofware_spec_uid\n",
    "    }\n",
    "\n",
    "    model_details = client.repository.store_model(\n",
    "        model=model_path,\n",
    "        meta_props=metadata\n",
    "    )\n",
    "    model_id = client.repository.get_model_id(model_details)\n",
    "else:\n",
    "    metadata = {\n",
    "            client.repository.ModelMetaNames.NAME: 'MNIST Tensorflow Model',\n",
    "    }\n",
    "\n",
    "    model_details = client.repository.update_model(\n",
    "        model_id,\n",
    "        updated_meta_props=metadata, update_model=model_path\n",
    "    )\n",
    "    model_id = client.repository.get_model_id(model_details)\n",
    "\n",
    "print(model_id)    \n",
    "stored_model_revision_details = client.repository.create_model_revision( model_uid=model_id)\n",
    "print(stored_model_revision_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model\n",
    "\n",
    "Andiamo a fare il deployment del modello creato.\n",
    "\n",
    "Se il deployment ID non viene passato o non viene trovato all'interno del deployment space, andiamo a crearne uno nuovo.\n",
    "\n",
    "Qualora si passasse un deployment ID già esistente, fa l'update del deployment aggiungendo l'asset con l'ultima revision creata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Change ID\n",
    "\n",
    "deployment_id = \"\"\n",
    "deployment_exist = False\n",
    "\n",
    "if deployment_id != \"\":\n",
    "    for entity in client.deployments.get_details()['resources']:\n",
    "        deployment_exist = entity['metadata']['id'] == deployment_id or deployment_exist\n",
    "\n",
    "    assert deployment_exist, \"ID Deployment non esiste\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy del modello\n",
    "rev = int(stored_model_revision_details['metadata']['rev'])\n",
    "if not deployment_exist:\n",
    "    metadata = {\n",
    "        client.deployments.ConfigurationMetaNames.NAME: \"MNIST Tensorflow Deployed Model\",\n",
    "        client.deployments.ConfigurationMetaNames.TAGS: ['OCR'],\n",
    "        client.deployments.ConfigurationMetaNames.ONLINE: {},\n",
    "        client.deployments.ConfigurationMetaNames.ASSET: {\"id\": model_id,\"rev\":str(rev)}\n",
    "    }\n",
    "\n",
    "    model_deployment_details = client.deployments.create(model_id, meta_props=metadata)\n",
    "    deployment_id = client.deployments.get_uid(model_deployment_details)\n",
    "    print(deployment_id)\n",
    "else:\n",
    "    metadata = {\n",
    "        client.deployments.ConfigurationMetaNames.ASSET: {\"id\": model_id,\"rev\":str(rev)}\n",
    "    }\n",
    "    model_deployment_details = client.deployments.update(deployment_id, changes=metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "while client.deployments.get_details(deployment_id)['entity']['status']['state']!='ready':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d91c8ca10844400c9393a9adecd64cb5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Deployment Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Testiamo il modello via API e via richieste HTTPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caba8bf6a4c344ff9e520f960e7e2b38",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing dati\n",
    "\n",
    "def getRGBAArr(canvas_data):\n",
    "    dimension = canvas_data[\"height\"]\n",
    "    rgba_data = canvas_data[\"data\"]\n",
    "    rgba_arr  = np.asarray(rgba_data).astype('uint8')\n",
    "    return rgba_arr.reshape(dimension, dimension, 4)\n",
    "\n",
    "def getNormAlphaList(img):\n",
    "    alpha_arr       = np.array(img.split()[-1])\n",
    "    norm_alpha_arr  = alpha_arr / 255\n",
    "    return norm_alpha_arr\n",
    "\n",
    "canvas_data   = sample_cavas_data     \n",
    "rgba_arr      = getRGBAArr(canvas_data)\n",
    "img           = Image.fromarray(rgba_arr, 'RGBA') \n",
    "sm_img        = img.resize((28, 28), Image.LANCZOS)\n",
    "alpha_list    = getNormAlphaList(sm_img)\n",
    "alpha_list    = alpha_list.reshape((1,28,28,1))\n",
    "model_payload = {\"input_data\": [{\"values\" : alpha_list}]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c8e390bd4da43b38a6cf5e15e885968",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict via API\n",
    "\n",
    "predictions = client.deployments.score(deployment_id, model_payload)\n",
    "print(json.dumps(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "771e39be29f846ff91817a31fa7a3f61",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict via HTTP request\n",
    "\n",
    "header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + cpdtoken}\n",
    "\n",
    "# NOTE: manually define and pass the array(s) of values to be scored in the next line\n",
    "payload_scoring = model_payload\n",
    "\n",
    "response_scoring = requests.post(f'https://cpd-cpd-instance.itzroks-666000zj44-mn5nm2-4b4a324f027aea19c5cbc0c3275c4656-0000.eu-de.containers.appdomain.cloud/ml/v4/deployments/{deployment_id}/predictions?version=2022-04-21', json=payload_scoring, headers=header, verify=False)\n",
    "print(\"Scoring response\")\n",
    "print(json.loads(response_scoring.text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
